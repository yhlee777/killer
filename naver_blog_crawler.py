# -*- coding: utf-8 -*-
# naver_blog_crawler.py - ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ + ê°€ê²Œ ë¶„ì„ (500ê°œ ìˆ˜ì§‘)

import requests
import json
import time
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from dataclasses import dataclass
from collections import Counter
import re
from datetime import datetime

# ==================== ë„¤ì´ë²„ API ì„¤ì • ====================

NAVER_CLIENT_ID = "ZLPHHehmKYVHcF2hUGhQ"  # ë„¤ì´ë²„ ê°œë°œìì„¼í„°ì—ì„œ ë°œê¸‰
NAVER_CLIENT_SECRET = "NrVaQLeDfV"


# ==================== ë°ì´í„° í´ë˜ìŠ¤ ====================

@dataclass
class BlogPost:
    """ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸"""
    title: str
    link: str
    description: str
    blogger_name: str
    post_date: str
    
    # ì¶”ì¶œëœ ì •ë³´
    keywords: List[str]
    sentiment: str  # "ê¸ì •", "ì¤‘ë¦½", "ë¶€ì •"
    visit_purpose: Optional[str]  # "ë°ì´íŠ¸", "ì¹œëª©", "í˜¼ìˆ " ë“±
    price_mentioned: Optional[int]
    menu_mentioned: List[str]


@dataclass
class StoreProfile:
    """ê°€ê²Œ í”„ë¡œí•„ (ë¸”ë¡œê·¸ì—ì„œ ìë™ ì¶”ì¶œ)"""
    name: str
    
    # ìë™ ì¶”ì¶œëœ ì •ë³´
    industry: str  # "ìš”ë¦¬ì£¼ì ", "ì´ìì¹´ì•¼", "ì¹´í˜" ë“±
    area: str  # "ê±´ëŒ€", "í™ëŒ€" ë“±
    concept: str  # "í—ŒíŒ…í¬ì°¨", "í˜¼ìˆ ì§‘", "ë°ì´íŠ¸" ë“±
    
    # ìƒì„¸ ë¶„ì„
    target_customers: List[str]  # ["20ëŒ€", "ëŒ€í•™ìƒ", "ì»¤í”Œ"]
    signature_menus: List[str]  # ì‹œê·¸ë‹ˆì²˜ ë©”ë‰´
    price_range: str  # "1-2ë§Œì›ëŒ€", "3-5ë§Œì›ëŒ€"
    atmosphere_keywords: List[str]  # ["ì‹œë„ëŸ¬ì›€", "ì•„ëŠ‘í•¨", "ë„“ìŒ"]
    visit_purposes: Dict[str, int]  # {"ë°ì´íŠ¸": 15, "ì¹œëª©": 30}
    
    # ìƒê¶Œ ë¶„ì„
    peak_times: List[str]  # ["ê¸ˆìš”ì¼ ì €ë…", "ì£¼ë§"]
    foot_traffic: str  # "ìƒ", "ì¤‘", "í•˜"
    competition_level: str  # "ë†’ìŒ", "ë³´í†µ", "ë‚®ìŒ"
    
    # í†µê³„
    total_blog_posts: int
    positive_ratio: float
    avg_rating: float


# ==================== ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ (500ê°œ ìˆ˜ì§‘) ====================

def search_naver_blog(query: str, total_count: int = 500) -> List[Dict]:
    """
    ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ API (ìµœëŒ€ 500ê°œ, ì¤‘ë³µ ì œê±°)
    
    Args:
        query: ê²€ìƒ‰ì–´ (ê°€ê²Œëª…)
        total_count: ê°€ì ¸ì˜¬ ì´ ê°œìˆ˜ (ê¸°ë³¸ 500ê°œ)
    
    Returns:
        ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (ì¤‘ë³µ ì œê±°ë¨)
    """
    url = "https://openapi.naver.com/v1/search/blog.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    
    all_blogs = []
    seen_links = set()  # ì¤‘ë³µ ì œê±°ìš©
    
    # 1ë‹¨ê³„: ì²« ìš”ì²­ìœ¼ë¡œ ì „ì²´ ê°œìˆ˜ í™•ì¸
    params = {
        "query": query,
        "display": 1,
        "sort": "sim"
    }
    
    try:
        response = requests.get(url, headers=headers, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
        available_total = data.get("total", 0)
        
        print(f"   ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: ì´ {available_total:,}ê°œ ë¸”ë¡œê·¸ ë°œê²¬")
        
        # ì‹¤ì œ ê°€ì ¸ì˜¬ ê°œìˆ˜ ê²°ì • (ìµœì†Œê°’)
        actual_count = min(total_count, available_total)
        
        if actual_count == 0:
            print("   âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            return []
        
        print(f"   ğŸ“¥ ìˆ˜ì§‘ ëª©í‘œ: {actual_count}ê°œ")
        
    except Exception as e:
        print(f"âŒ ì´ˆê¸° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []
    
    # 2ë‹¨ê³„: 100ê°œì”© ë‚˜ëˆ ì„œ ìš”ì²­
    max_per_request = 100
    requests_needed = (actual_count + max_per_request - 1) // max_per_request  # ì˜¬ë¦¼
    
    for page in range(requests_needed):
        start = page * max_per_request + 1
        
        # ë‚¨ì€ ê°œìˆ˜ ê³„ì‚°
        remaining = actual_count - len(all_blogs)
        if remaining <= 0:
            break
        
        # ì´ë²ˆ ìš”ì²­ì—ì„œ ê°€ì ¸ì˜¬ ê°œìˆ˜
        display = min(max_per_request, remaining)
        
        params = {
            "query": query,
            "display": display,
            "start": start,
            "sort": "sim"
        }
        
        try:
            print(f"   â³ ìš”ì²­ {page + 1}/{requests_needed}: start={start}, display={display}")
            
            response = requests.get(url, headers=headers, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()
            items = data.get("items", [])
            
            # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ì¶”ê°€
            new_count = 0
            for item in items:
                link = item.get("link", "")
                if link and link not in seen_links:
                    seen_links.add(link)
                    all_blogs.append(item)
                    new_count += 1
                    
                    # ëª©í‘œ ê°œìˆ˜ ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
                    if len(all_blogs) >= actual_count:
                        break
            
            print(f"      âœ… ìˆ˜ì§‘: {new_count}ê°œ (ëˆ„ì : {len(all_blogs)}/{actual_count}ê°œ)")
            
            # ëª©í‘œ ê°œìˆ˜ ë„ë‹¬í•˜ë©´ ë£¨í”„ ì¢…ë£Œ
            if len(all_blogs) >= actual_count:
                break
            
            # API ìš”ì²­ ê°„ê²© (ê³¼ë¶€í•˜ ë°©ì§€)
            if page < requests_needed - 1:
                time.sleep(0.1)
                
        except Exception as e:
            print(f"   âš ï¸  ìš”ì²­ {page + 1} ì‹¤íŒ¨: {e}")
            continue
    
    print(f"   âœ… ìµœì¢… ìˆ˜ì§‘: {len(all_blogs)}ê°œ (ì¤‘ë³µ ì œê±° ì™„ë£Œ)")
    return all_blogs


# ==================== ë¸”ë¡œê·¸ ë‚´ìš© í¬ë¡¤ë§ ====================

def crawl_blog_content(blog_url: str) -> str:
    """
    ë¸”ë¡œê·¸ ë³¸ë¬¸ í¬ë¡¤ë§
    
    Args:
        blog_url: ë¸”ë¡œê·¸ URL
    
    Returns:
        ë³¸ë¬¸ í…ìŠ¤íŠ¸
    """
    try:
        # User-Agent ì„¤ì •
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        }
        
        response = requests.get(blog_url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë³¸ë¬¸ ì¶”ì¶œ
        # iframe ë‚´ë¶€ ì ‘ê·¼ í•„ìš” (ì‹¤ì œë¡œëŠ” ë” ë³µì¡)
        content = soup.get_text(strip=True)
        
        return content
    except Exception as e:
        print(f"âš ï¸  í¬ë¡¤ë§ ì‹¤íŒ¨ ({blog_url}): {e}")
        return ""


# ==================== í‚¤ì›Œë“œ ì¶”ì¶œ ====================

def extract_keywords(text: str) -> List[str]:
    """
    í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
    """
    # ì—…ì¢… í‚¤ì›Œë“œ
    industry_keywords = [
        "ìš”ë¦¬ì£¼ì ", "ìˆ ì§‘", "ì£¼ì ", "í˜¸í”„ì§‘", "í¬ì°¨", "ì´ìì¹´ì•¼", 
        "ì™€ì¸ë°”", "ì¹µí…Œì¼ë°”", "í", "ë°”", "ì¹´í˜", "ìŒì‹ì "
    ]
    
    # ì»¨ì…‰ í‚¤ì›Œë“œ
    concept_keywords = [
        "í—ŒíŒ…", "ë¯¸íŒ…", "ì†Œê°œíŒ…", "ë°ì´íŠ¸", "í˜¼ìˆ ", "ì¹œëª©", 
        "íšŒì‹", "ë‹¨ì²´", "ì¡°ìš©í•œ", "ì‹œë„ëŸ¬ìš´", "ë¶„ìœ„ê¸°"
    ]
    
    # ë§› í‚¤ì›Œë“œ
    taste_keywords = [
        "ë§›ìˆë‹¤", "ë§›ì—†ë‹¤", "ë‹¬ë‹¤", "ì§œë‹¤", "ë§¤ì½¤", "ê³ ì†Œ", 
        "ì‹ ì„ ", "ë°”ì‚­", "ë¶€ë“œëŸ½", "ì«„ê¹ƒ"
    ]
    
    # ì„œë¹„ìŠ¤ í‚¤ì›Œë“œ
    service_keywords = [
        "ì¹œì ˆ", "ë¶ˆì¹œì ˆ", "ë¹ ë¥´ë‹¤", "ëŠë¦¬ë‹¤", "ì„¸ì‹¬", "ë¶ˆí¸"
    ]
    
    found_keywords = []
    text_lower = text.lower()
    
    for keyword in industry_keywords + concept_keywords + taste_keywords + service_keywords:
        if keyword in text_lower:
            found_keywords.append(keyword)
    
    return found_keywords


# ==================== ê°ì • ë¶„ì„ (ê°„ë‹¨ ë²„ì „) ====================

def analyze_sentiment(text: str) -> str:
    """
    ê°„ë‹¨í•œ ê°ì • ë¶„ì„
    """
    positive_words = ["ì¢‹", "ìµœê³ ", "ë§›ìˆ", "ì¹œì ˆ", "ê¹¨ë—", "ì¶”ì²œ"]
    negative_words = ["ë‚˜ì˜", "ë³„ë¡œ", "ë§›ì—†", "ë¶ˆì¹œì ˆ", "ë”ëŸ½", "ë¹„ì¶”"]
    
    pos_count = sum(1 for word in positive_words if word in text)
    neg_count = sum(1 for word in negative_words if word in text)
    
    if pos_count > neg_count:
        return "ê¸ì •"
    elif neg_count > pos_count:
        return "ë¶€ì •"
    else:
        return "ì¤‘ë¦½"


# ==================== ê°€ê²Œ í”„ë¡œí•„ ë¶„ì„ (ë¸”ë¡œê·¸ì—ì„œ) ====================

def analyze_store_from_blog(store_name: str, max_blogs: int = 500) -> StoreProfile:
    """
    ë¸”ë¡œê·¸ ë°ì´í„°ë¡œë¶€í„° ê°€ê²Œ í”„ë¡œí•„ ìë™ ì¶”ì¶œ
    
    Args:
        store_name: ê°€ê²Œëª…
        max_blogs: ë¶„ì„í•  ìµœëŒ€ ë¸”ë¡œê·¸ ìˆ˜
    
    Returns:
        StoreProfile ê°ì²´
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“± ë¸”ë¡œê·¸ ë¶„ì„ ì‹œì‘: {store_name}")
    print(f"{'='*60}")
    
    # ë¸”ë¡œê·¸ ê²€ìƒ‰
    blogs = search_naver_blog(store_name, total_count=max_blogs)
    
    if not blogs:
        raise Exception("ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
    
    print(f"\n   ğŸ” ì´ {len(blogs)}ê°œ ë¸”ë¡œê·¸ ë¶„ì„ ì¤‘...")
    
    # í‚¤ì›Œë“œ ì¹´ìš´í„°
    all_keywords = []
    sentiments = []
    visit_purposes = []
    
    for blog in blogs:
        title = blog.get('title', '')
        description = blog.get('description', '')
        combined_text = title + " " + description
        
        # HTML íƒœê·¸ ì œê±°
        combined_text = re.sub(r'<[^>]+>', '', combined_text)
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = extract_keywords(combined_text)
        all_keywords.extend(keywords)
        
        # ê°ì • ë¶„ì„
        sentiment = analyze_sentiment(combined_text)
        sentiments.append(sentiment)
        
        # ë°©ë¬¸ ëª©ì  ì¶”ì¶œ
        if "ë°ì´íŠ¸" in combined_text or "ì†Œê°œíŒ…" in combined_text:
            visit_purposes.append("ë°ì´íŠ¸")
        elif "íšŒì‹" in combined_text or "ë‹¨ì²´" in combined_text:
            visit_purposes.append("íšŒì‹")
        elif "í˜¼ìˆ " in combined_text:
            visit_purposes.append("í˜¼ìˆ ")
        elif "ì¹œëª©" in combined_text or "ì¹œêµ¬" in combined_text:
            visit_purposes.append("ì¹œëª©")
    
    # í†µê³„ ì§‘ê³„
    keyword_counter = Counter(all_keywords)
    purpose_counter = Counter(visit_purposes)
    
    # ì—…ì¢… ì¶”ì •
    industry_candidates = ["ìš”ë¦¬ì£¼ì ", "ìˆ ì§‘", "ì¹´í˜", "ì´ìì¹´ì•¼", "ë°”"]
    industry = "ì¼ë°˜"
    for candidate in industry_candidates:
        if keyword_counter.get(candidate, 0) > 0:
            industry = candidate
            break
    
    # ì»¨ì…‰ ì¶”ì •
    concept_candidates = ["í—ŒíŒ…í¬ì°¨", "ë°ì´íŠ¸", "í˜¼ìˆ ì§‘", "íšŒì‹"]
    concept = "ì¼ë°˜"
    for candidate in concept_candidates:
        if keyword_counter.get(candidate, 0) > 2:
            concept = candidate
            break
    
    # ì§€ì—­ ì¶”ì • (ê°„ë‹¨ ë²„ì „)
    area = "ë¯¸ìƒ"
    area_keywords = ["ê±´ëŒ€", "í™ëŒ€", "ê°•ë‚¨", "ì‹ ì´Œ", "ì´íƒœì›", "ëª…ë™"]
    for area_kw in area_keywords:
        if any(area_kw in blog.get('title', '') + blog.get('description', '') for blog in blogs[:10]):
            area = area_kw
            break
    
    # ê¸ì • ë¹„ìœ¨
    positive_count = sentiments.count("ê¸ì •")
    positive_ratio = positive_count / len(sentiments) if sentiments else 0.0
    
    # í‰ê·  í‰ì  (ê°„ë‹¨ ì¶”ì •)
    avg_rating = 3.0 + (positive_ratio * 2.0)  # 3.0 ~ 5.0
    
    # StoreProfile ìƒì„±
    profile = StoreProfile(
        name=store_name,
        industry=industry,
        area=area,
        concept=concept,
        target_customers=["20ëŒ€", "ëŒ€í•™ìƒ"],  # ê°„ë‹¨ ì¶”ì •
        signature_menus=[],
        price_range="2-3ë§Œì›ëŒ€",
        atmosphere_keywords=[kw for kw, _ in keyword_counter.most_common(5)],
        visit_purposes=dict(purpose_counter),
        peak_times=["ê¸ˆìš”ì¼ ì €ë…", "ì£¼ë§"],
        foot_traffic="ì¤‘",
        competition_level="ë†’ìŒ",
        total_blog_posts=len(blogs),
        positive_ratio=positive_ratio,
        avg_rating=avg_rating
    )
    
    print(f"   âœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"      ì—…ì¢…: {profile.industry}")
    print(f"      ì»¨ì…‰: {profile.concept}")
    print(f"      ê¸ì • ë¹„ìœ¨: {profile.positive_ratio:.1%}")
    
    return profile


# ==================== ë©”ì¸ ì‹¤í–‰ ====================

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    store_name = input("ê°€ê²Œ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    
    if store_name:
        profile = analyze_store_from_blog(store_name, max_blogs=500)
        print(f"\n{'='*60}")
        print(json.dumps(profile.__dict__, ensure_ascii=False, indent=2))
        print(f"{'='*60}")