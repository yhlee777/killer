# -*- coding: utf-8 -*-
# turbo_crawler.py - ë´‡ íƒì§€ ìš°íšŒ + headless=False

import asyncio
import sqlite3
import re
import json
import time
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright
import random

DB_FILE = 'seoul_industry_reviews.db'
PROGRESS_FILE = "turbo_progress.json"
PARALLEL_WORKERS = 5  # ğŸ”¥ 5ê°œ ì›Œì»¤ (ì†ë„ UP)
TARGET_REVIEWS = 100
SCROLL_DEPTH = 15

# ==================== User-Agent ëœë¤í™” ====================

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0"
]

# ==================== ì„œìš¸ ì§€ì—­ ====================

SEOUL_ALL_AREAS = [
    "ê°•ë‚¨ì—­", "ì‹ ë…¼í˜„ì—­", "ë…¼í˜„ì—­", "ì••êµ¬ì •", "ì²­ë‹´", "ì‚¼ì„±ë™", "ì—­ì‚¼", "ì„ ë¦‰",
    "í™ëŒ€ì…êµ¬ì—­", "í™ëŒ€", "ì—°ë‚¨ë™", "ìƒìˆ˜ì—­", "í•©ì •ì—­", "ë§ì›ë™",
    "ì´íƒœì›", "ê²½ë¦¬ë‹¨ê¸¸", "í•œë‚¨ë™", "ì„±ìˆ˜ë™", "ëšì„¬", "ê±´ëŒ€ì…êµ¬",
    "ì‹ ì´Œ", "ì´ëŒ€", "ì—¬ì˜ë„", "ì ì‹¤", "í˜œí™”", "ì„±ì‹ ì—¬ëŒ€",
    "ì¢…ë¡œ", "ëª…ë™", "ìµì„ ë™", "ì‚¼ì²­ë™"
]

INDUSTRIES = [
    "ì¹´í˜", "ë””ì €íŠ¸ì¹´í˜", "ë² ì´ì»¤ë¦¬", "ë¸ŒëŸ°ì¹˜ì¹´í˜",
    "íŒŒìŠ¤íƒ€", "ì´íƒˆë¦¬ì•ˆ", "ìŠ¤í…Œì´í¬", "í”¼ì", "ë²„ê±°",
    "ì˜¤ë§ˆì¹´ì„¸", "ìŠ¤ì‹œ", "ì´ìì¹´ì•¼", "ë¼ë©˜", "ìš°ë™", "ëˆì¹´ì¸ ",
    "ë”¤ì„¬", "ë§ˆë¼íƒ•", "ìŒ€êµ­ìˆ˜", "íƒœêµ­ìŒì‹",
    "í•œì‹ë‹¹", "ê³ ê¸°ì§‘", "ì‚¼ê²¹ì‚´", "ê³±ì°½",
    "ì™€ì¸ë°”", "ì¹µí…Œì¼ë°”", "ë°”", "í"
]

# ==================== DB ê´€ë ¨ ====================

def get_existing_place_ids():
    try:
        conn = sqlite3.connect(DB_FILE)
        cursor = conn.cursor()
        cursor.execute("SELECT place_id FROM stores")
        existing = set(row[0] for row in cursor.fetchall())
        conn.close()
        print(f"âœ… ê¸°ì¡´ ê°€ê²Œ: {len(existing):,}ê°œ")
        return existing
    except:
        return set()

def save_to_db(place_id, store_name, region, industry, reviews):
    try:
        conn = sqlite3.connect(DB_FILE, timeout=30)
        cursor = conn.cursor()
        
        cursor.execute("""
            INSERT OR REPLACE INTO stores 
            (place_id, name, district, industry, review_count, crawled_at) 
            VALUES (?, ?, ?, ?, ?, ?)
        """, (place_id, store_name, region, industry, len(reviews), datetime.now().isoformat()))
        
        for r in reviews:
            cursor.execute("""
                INSERT INTO reviews 
                (place_id, date, content, crawled_at) 
                VALUES (?, ?, ?, ?)
            """, (place_id, r.get('ë‚ ì§œ'), r.get('ë¦¬ë·°', ''), datetime.now().isoformat()))
        
        conn.commit()
        conn.close()
        return True
    except:
        return False

def save_progress_safe(progress):
    try:
        with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)
        return True
    except:
        return False

# ==================== í¬ë¡¤ë§ ====================

def is_owner_reply(review_text):
    if not review_text:
        return False
    strong_signals = ['ë¦¬ë·° ê°ì‚¬', 'ì§„ì‹¬ìœ¼ë¡œ ê°ì‚¬', 'ì°¾ì•„ì£¼ì…”ì„œ', 'ë³´ë‹µí•˜ê² ìŠµë‹ˆë‹¤']
    return any(s in review_text for s in strong_signals) or \
           sum(1 for k in ['ê°ì‚¬ë“œë¦½ë‹ˆë‹¤', 'ê°ì‚¬í•©ë‹ˆë‹¤'] if k in review_text) >= 2

async def expand_reviews(page):
    try:
        await page.evaluate("""
            () => {
                document.querySelectorAll('button, a').forEach(btn => {
                    if (btn.textContent.includes('í¼ì³ì„œ ë”ë³´ê¸°')) {
                        try { btn.click(); } catch(e) {}
                    }
                });
            }
        """)
    except:
        pass

async def collect_place_ids(search_query, scroll_depth, max_stores, worker_id, existing_ids):
    """Place ID ìˆ˜ì§‘ - ë´‡ íƒì§€ ìš°íšŒ"""
    async with async_playwright() as playwright:
        browser = await playwright.chromium.launch(
            headless=False,  # ğŸ”¥ False ìœ ì§€!
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-web-security'
            ]
        )
        
        # ğŸ”¥ ëœë¤ User-Agent
        user_agent = random.choice(USER_AGENTS)
        
        context = await browser.new_context(
            viewport={"width": 1920, "height": 1080}, 
            locale="ko-KR",
            user_agent=user_agent,
            # ğŸ”¥ ì¶”ê°€ í—¤ë”
            extra_http_headers={
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }
        )
        
        # ğŸ”¥ ê°•í™”ëœ ë´‡ íƒì§€ ìš°íšŒ
        await context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
            Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
            Object.defineProperty(navigator, 'languages', { get: () => ['ko-KR', 'ko', 'en-US', 'en'] });
            window.chrome = { runtime: {} };
            Object.defineProperty(navigator, 'permissions', {
                get: () => ({
                    query: () => Promise.resolve({ state: 'granted' })
                })
            });
        """)
        
        page = await context.new_page()
        
        try:
            # ğŸ”¥ ëœë¤ ëŒ€ê¸° (1-3ì´ˆ)
            await asyncio.sleep(random.uniform(1, 3))
            
            await page.goto(
                f"https://map.naver.com/v5/search/{search_query}", 
                wait_until="domcontentloaded", 
                timeout=30000
            )
            await asyncio.sleep(random.uniform(2, 4))  # ğŸ”¥ ëœë¤ ëŒ€ê¸°
            
            search_frame = None
            for attempt in range(12):
                for frame in page.frames:
                    if "searchIframe" in frame.name or "entry=plt" in frame.url or "restaurant/list" in frame.url:
                        search_frame = frame
                        break
                if search_frame:
                    break
                await asyncio.sleep(random.uniform(0.5, 1.0))
            
            if not search_frame:
                await context.close()
                await browser.close()
                return []
            
            # ğŸ”¥ ì¸ê°„ì²˜ëŸ¼ ìŠ¤í¬ë¡¤ (ì†ë„ ëœë¤)
            for i in range(scroll_depth):
                try:
                    scroll_amount = random.randint(700, 1000)
                    await search_frame.evaluate(f"window.scrollBy(0, {scroll_amount})")
                    await asyncio.sleep(random.uniform(0.3, 0.6))
                except:
                    pass
            
            await asyncio.sleep(1)
            
            store_items = []
            for sel in ["li.UEzoS", "li.CHC5F", "li[data-id]"]:
                try:
                    items = await search_frame.locator(sel).all()
                    valid_items = []
                    for item in items[:50]:
                        try:
                            text = await item.inner_text(timeout=300)
                            if len(text) > 10:
                                valid_items.append(item)
                        except:
                            pass
                    if len(valid_items) >= 5:
                        store_items = valid_items
                        break
                except:
                    pass
            
            place_data = []
            for i, item in enumerate(store_items[:max_stores], 1):
                try:
                    store_name = "ì•Œ ìˆ˜ ì—†ìŒ"
                    try:
                        name_text = await item.inner_text(timeout=500)
                        lines = [ln.strip() for ln in name_text.split('\n') if ln.strip()]
                        if lines:
                            store_name = lines[0][:30]
                    except:
                        pass
                    
                    # ğŸ”¥ ëœë¤ ëŒ€ê¸° (ì¸ê°„ ì‹œë®¬ë ˆì´ì…˜)
                    await asyncio.sleep(random.uniform(0.3, 0.8))
                    await item.click(timeout=2000)
                    await asyncio.sleep(random.uniform(1, 2))
                    
                    place_id = None
                    for frame in page.frames:
                        match = re.search(r'place[/=](\d+)', frame.url)
                        if match:
                            place_id = match.group(1)
                            break
                    
                    if place_id and place_id not in existing_ids:
                        place_data.append({"place_id": place_id, "name": store_name})
                    
                    await page.go_back(wait_until="domcontentloaded", timeout=8000)
                    await asyncio.sleep(random.uniform(0.5, 1.0))
                    
                    search_frame = None
                    for frame in page.frames:
                        if "searchIframe" in frame.name or "entry=plt" in frame.url:
                            search_frame = frame
                            break
                    if not search_frame:
                        break
                    
                except:
                    continue
            
            await context.close()
            await browser.close()
            return place_data
            
        except:
            await context.close()
            await browser.close()
            return []

async def collect_reviews(place_id, store_name, region, industry, worker_id, existing_ids):
    """ë¦¬ë·° ìˆ˜ì§‘ - ë´‡ íƒì§€ ìš°íšŒ"""
    if place_id in existing_ids:
        return None
    
    async with async_playwright() as playwright:
        browser = await playwright.chromium.launch(
            headless=False,  # ğŸ”¥ False ìœ ì§€!
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage'
            ]
        )
        
        user_agent = random.choice(USER_AGENTS)
        
        context = await browser.new_context(
            viewport={"width": 1280, "height": 900}, 
            locale="ko-KR",
            user_agent=user_agent
        )
        
        await context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
            window.chrome = { runtime: {} };
        """)
        
        page = await context.new_page()
        
        try:
            await asyncio.sleep(random.uniform(0.5, 1.5))
            
            review_url = f"https://m.place.naver.com/restaurant/{place_id}/review/visitor"
            await page.goto(review_url, wait_until="domcontentloaded", timeout=30000)
            await asyncio.sleep(random.uniform(1.5, 2.5))
            
            # ìµœì‹ ìˆœ ì •ë ¬
            try:
                for selector in ["button:has-text('ìµœì‹ ìˆœ')", "a:has-text('ìµœì‹ ìˆœ')"]:
                    try:
                        await page.click(selector, timeout=2000)
                        await asyncio.sleep(1)
                        break
                    except:
                        continue
            except:
                pass
            
            # ğŸ”¥ ì¸ê°„ì²˜ëŸ¼ ìŠ¤í¬ë¡¤
            for i in range(25):
                try:
                    scroll_amount = random.randint(1800, 2200)
                    await page.evaluate(f"window.scrollBy(0, {scroll_amount})")
                except:
                    pass
                await asyncio.sleep(random.uniform(0.2, 0.4))
                await expand_reviews(page)
                
                if i % 10 == 0:
                    count = 0
                    for sel in ["li.place_apply_pui", "li.pui__X35jYm"]:
                        try:
                            count = await page.locator(sel).count()
                            if count >= TARGET_REVIEWS:
                                break
                        except:
                            pass
                    if count >= TARGET_REVIEWS:
                        break
            
            reviews = []
            review_items = []
            for sel in ["li.place_apply_pui", "li.pui__X35jYm", "li.EjjAW"]:
                try:
                    items = await page.locator(sel).all()
                    if items:
                        review_items = items
                        break
                except:
                    pass
            
            for item in review_items[:TARGET_REVIEWS + 20]:
                try:
                    full_text = await item.inner_text(timeout=500)
                    if len(full_text) < 50 or 'í‚¤ì›Œë“œÂ·ë³„ì ' in full_text:
                        continue
                    
                    review = {"ë³„ì ": None, "ë‚ ì§œ": None, "ë¦¬ë·°": ""}
                    
                    if 'â˜…' in full_text:
                        review["ë³„ì "] = float(min(full_text.count('â˜…'), 5))
                    
                    date_patterns = [
                        r'(\d{4}[./-]\d{1,2}[./-]\d{1,2})',
                        r'(\d{1,2}[./-]\d{1,2})',
                        r'(\d+ì¼\s*ì „)',
                    ]
                    
                    for pattern in date_patterns:
                        date_match = re.search(pattern, full_text)
                        if date_match:
                            review["ë‚ ì§œ"] = date_match.group(1)
                            break
                    
                    lines = [ln.strip() for ln in full_text.split('\n') if len(ln.strip()) >= 20]
                    lines = [ln for ln in lines if not re.search(r'\d{4}[./-]', ln)]
                    if lines:
                        review["ë¦¬ë·°"] = max(lines, key=len)
                    
                    if review["ë¦¬ë·°"] and len(review["ë¦¬ë·°"]) >= 20 and not is_owner_reply(review["ë¦¬ë·°"]):
                        reviews.append(review)
                    
                    if len(reviews) >= TARGET_REVIEWS:
                        break
                        
                except:
                    pass
            
            await context.close()
            await browser.close()
            return reviews
            
        except:
            await context.close()
            await browser.close()
            return []

# ==================== ì›Œì»¤ ====================

async def worker(worker_id, query_queue, progress, existing_ids, stats_lock):
    print(f"[W{worker_id}] ğŸš€ ì›Œì»¤ ì‹œì‘")
    
    while True:
        try:
            query_data = await query_queue.get()
            if query_data is None:
                break
            
            query = query_data['query']
            area = query_data['area']
            industry = query_data['industry']
            
            print(f"\n[W{worker_id}] ğŸ¯ {query}")
            
            # ğŸ”¥ ì›Œì»¤ë§ˆë‹¤ ì‹œì‘ ì‹œê°„ ëœë¤í™” (ë´‡ íƒì§€ íšŒí”¼)
            await asyncio.sleep(random.uniform(0, 3))
            
            place_data = await collect_place_ids(query, SCROLL_DEPTH, 20, worker_id, existing_ids)
            
            if not place_data:
                print(f"[W{worker_id}] âš ï¸  ì‹ ê·œ ì—†ìŒ")
                async with stats_lock:
                    progress["completed_queries"].append(query)
                    save_progress_safe(progress)
                query_queue.task_done()
                continue
            
            for i, store in enumerate(place_data, 1):
                place_id = store['place_id']
                store_name = store['name']
                
                reviews = await collect_reviews(place_id, store_name, area, industry, worker_id, existing_ids)
                
                if reviews:
                    save_to_db(place_id, store_name, area, industry, reviews)
                    existing_ids.add(place_id)
                    
                    async with stats_lock:
                        progress["new_stores_count"] = progress.get("new_stores_count", 0) + 1
                        progress["total_reviews"] = progress.get("total_reviews", 0) + len(reviews)
                        save_progress_safe(progress)
                    
                    print(f"[W{worker_id}] ğŸ’¾ [{i}/{len(place_data)}] {store_name[:15]} - {len(reviews)}ê°œ")
            
            async with stats_lock:
                progress["completed_queries"].append(query)
                save_progress_safe(progress)
            
            query_queue.task_done()
            
        except Exception as e:
            print(f"[W{worker_id}] âŒ {e}")
            query_queue.task_done()

# ==================== ë©”ì¸ ====================

async def main():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   ğŸ›¡ï¸ í„°ë³´ í¬ë¡¤ëŸ¬ v2 (ë´‡ íƒì§€ ìš°íšŒ)                 â•‘
â•‘   - ì›Œì»¤ 5ê°œ                                         â•‘
â•‘   - headless=False (ì•ˆì „)                            â•‘
â•‘   - ëœë¤ User-Agent                                  â•‘
â•‘   - ì¸ê°„ ì‹œë®¬ë ˆì´ì…˜ (ëœë¤ ëŒ€ê¸°)                      â•‘
â•‘   - ì‹¤ì‹œê°„ progress ì €ì¥                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    existing_ids = get_existing_place_ids()
    
    queries = []
    for area in SEOUL_ALL_AREAS:
        for industry in INDUSTRIES:
            queries.append({'query': f"{area} {industry}", 'area': area, 'industry': industry})
    random.shuffle(queries)
    
    print(f"ğŸ“‹ ì´ ì¿¼ë¦¬: {len(queries):,}ê°œ\n")
    
    progress = {"completed_queries": [], "new_stores_count": 0, "total_reviews": 0}
    if Path(PROGRESS_FILE).exists():
        try:
            with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
                progress = json.load(f)
            print(f"ğŸ“‚ ì´ì „ ì§„í–‰: {len(progress['completed_queries'])}ê°œ ì™„ë£Œ")
            print(f"   ì‹ ê·œ ê°€ê²Œ: {progress.get('new_stores_count', 0)}ê°œ")
            print(f"   ì‹ ê·œ ë¦¬ë·°: {progress.get('total_reviews', 0):,}ê°œ\n")
        except:
            pass
    
    query_queue = asyncio.Queue()
    for q in queries:
        if q['query'] not in progress["completed_queries"]:
            await query_queue.put(q)
    
    print(f"ğŸ¯ ë‚¨ì€ ì¿¼ë¦¬: {query_queue.qsize():,}ê°œ\n")
    
    for _ in range(PARALLEL_WORKERS):
        await query_queue.put(None)
    
    stats_lock = asyncio.Lock()
    workers = [
        asyncio.create_task(worker(i, query_queue, progress, existing_ids, stats_lock)) 
        for i in range(PARALLEL_WORKERS)
    ]
    
    await query_queue.join()
    await asyncio.gather(*workers)
    
    save_progress_safe(progress)
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   âœ… ì™„ë£Œ!                                           â•‘
â•‘   ğŸ†• ì‹ ê·œ ê°€ê²Œ: {progress['new_stores_count']:,}ê°œ
â•‘   ğŸ“ ì‹ ê·œ ë¦¬ë·°: {progress.get('total_reviews', 0):,}ê°œ
â•‘   ğŸ“Š ì™„ë£Œ ì¿¼ë¦¬: {len(progress['completed_queries']):,}ê°œ
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

if __name__ == "__main__":
    asyncio.run(main())